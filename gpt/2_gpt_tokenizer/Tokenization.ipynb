{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a12ecf9a-5835-4b88-b957-efe4b4bbef22",
   "metadata": {},
   "source": [
    "# Tokenization :)\n",
    "\n",
    "Tokenization is at the heart of much weirdness of the LLMs. Do not brush it off.\n",
    "\n",
    "* Why can't LLM spell words? **Tokenization.**\n",
    "* Why can't LLM do super simple string processing tasks like reversing a string? **Tokenization.**\n",
    "* Why is LLM worse at non-English languages (e.g. Japanese? **Tokenization.**\n",
    "* Why is LLM bad at simple arithmetic? **Tokenization.**\n",
    "* Why did GPT-2 have more than necessary trouble coding in Python? **Tokenization.**\n",
    "* Why did my LLM abruptly halt when it sees the string <|endoftext|>? **Tokenization.**\n",
    "* what is this weird warning I get about a \"trailing whitespace\"? **Tokenization.**\n",
    "* Why did the LLM break when I ask it about \"SolidGoldMagikarp\"? **Tokenization.**\n",
    "* Why should i prefer to use YAML over JSON with LLMs? **Tokenization.**\n",
    "* Why is the LLM not actually end-to-end language modelling? **Tokenization.**\n",
    "* What is the real root of suffering? **Tokenization.**\n",
    "\n",
    "---\n",
    "\n",
    "Good tokenization web app: [https://tiktokenizer.vercel.app](https://tiktokenizer.vercel.app)\n",
    "\n",
    "Example string:\n",
    "\n",
    "```\n",
    "Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
    "\n",
    "127 + 677 = 804\n",
    "1275 + 6773 = 8041\n",
    "\n",
    "Egg.\n",
    "I have an Egg.\n",
    "egg.\n",
    "EGG.\n",
    "\n",
    "\n",
    "만나서 반가워요. 저는 OpenAI에서 개발한 대규모 언어 모델인 ChatGPT입니다. 궁금한 것이 있으시면 무엇이든 물어보세요.\n",
    "\n",
    "for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Much glory awaits someone who can delete the need for tokenization. But meanwhile, let's learn about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e813507e-7711-47bb-97bc-4a12be988f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요 👋 (hello in Korean!)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"안녕하세요 👋 (hello in Korean!)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e765b77-ec15-42db-bfc4-d5ec3d1ea76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50504,\n",
       " 45397,\n",
       " 54616,\n",
       " 49464,\n",
       " 50836,\n",
       " 32,\n",
       " 128075,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x) for x in \"안녕하세요 👋 (hello in Korean!)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26f41a18-4ed2-42fb-b738-37d620855e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[236,\n",
       " 149,\n",
       " 136,\n",
       " 235,\n",
       " 133,\n",
       " 149,\n",
       " 237,\n",
       " 149,\n",
       " 152,\n",
       " 236,\n",
       " 132,\n",
       " 184,\n",
       " 236,\n",
       " 154,\n",
       " 148,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 145,\n",
       " 139,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"안녕하세요 👋 (hello in Korean!)\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e4eaa040-e2bb-4765-a92c-9a95cc789518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\n",
      "length:  533\n",
      "---\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "length:  616\n"
     ]
    }
   ],
   "source": [
    "text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\"\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))\n",
    "print(\"---\")\n",
    "print(text)\n",
    "print(\"length: \", len(text))\n",
    "print(\"---\")\n",
    "print(tokens)\n",
    "print(\"length: \", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7c373de-39b4-4ed8-9987-47d1f6c01e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndic = {}\\nfor i in range(len(tokens)-1):\\n    pair = tuple([tokens[i], tokens[i+1]])\\n    if pair in dic:\\n        dic[pair] += 1\\n    else:\\n        dic[pair] = 1\\n\\ndic_sorted = sorted(dic.items(), key= lambda x:x[1], reverse=True)\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my way\n",
    "\"\"\"\n",
    "dic = {}\n",
    "for i in range(len(tokens)-1):\n",
    "    pair = tuple([tokens[i], tokens[i+1]])\n",
    "    if pair in dic:\n",
    "        dic[pair] += 1\n",
    "    else:\n",
    "        dic[pair] = 1\n",
    "\n",
    "dic_sorted = sorted(dic.items(), key= lambda x:x[1], reverse=True)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b682a0db-4ac1-4f79-824d-7261ae555ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andrej\n",
    "\n",
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): \n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "# print(sorted(((v,k) for k,v in stats.items()), reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af61136e-bf24-4720-b2ec-2cbcdbe36ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3173450-d17e-4be5-ace5-3e1e167f95a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 256, 118, 101, 114, 121, 32, 110, 97, 109, 256, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 256, 105, 110, 116, 111, 32, 116, 104, 256, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 256, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 256, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 256, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 256, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 256, 99, 97, 110, 32, 98, 256, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 256, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 256, 85, 110, 105, 99, 111, 100, 256, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 256, 109, 111, 114, 256, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 256, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 256, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 256, 119, 104, 111, 108, 256, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "length:  596\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if we are not at the very last position AND the pair matches --> replace it\n",
    "        if (i < len(ids) - 1) and (ids[i] == pair[0] and ids[i+1] == pair[1]):\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "# print(merge([5,6,6,7,9,1], (6,7), 99))\n",
    "\n",
    "tokens2 = merge(tokens,top_pair, 256)\n",
    "print(tokens2)\n",
    "print(\"length: \", len(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6df4a19e-344c-4ff0-9e2b-be77589ceac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the training text longer to have more representative token statistics\n",
    "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text = \"\"\"A Programmer’s Introduction to Unicode March 3, 2017 · Coding · 22 Comments  Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺\\u200c🇳\\u200c🇮\\u200c🇨\\u200c🇴\\u200c🇩\\u200c🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.  A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, I’ll give an introduction to it from a programmer’s point of view.  I’m going to focus on the character set and what’s involved in working with strings and files of Unicode text. However, in this article I’m not going to talk about fonts, text layout/shaping/rendering, or localization in detail—those are separate issues, beyond my scope (and knowledge) here.  Diversity and Inherent Complexity The Unicode Codespace Codespace Allocation Scripts Usage Frequency Encodings UTF-8 UTF-16 Combining Marks Canonical Equivalence Normalization Forms Grapheme Clusters And More… Diversity and Inherent Complexity As soon as you start to study Unicode, it becomes clear that it represents a large jump in complexity over character sets like ASCII that you may be more familiar with. It’s not just that Unicode contains a much larger number of characters, although that’s part of it. Unicode also has a great deal of internal structure, features, and special cases, making it much more than what one might expect a mere “character set” to be. We’ll see some of that later in this article.  When confronting all this complexity, especially as an engineer, it’s hard not to find oneself asking, “Why do we need all this? Is this really necessary? Couldn’t it be simplified?”  However, Unicode aims to faithfully represent the entire world’s writing systems. The Unicode Consortium’s stated goal is “enabling people around the world to use computers in any language”. And as you might imagine, the diversity of written languages is immense! To date, Unicode supports 135 different scripts, covering some 1100 languages, and there’s still a long tail of over 100 unsupported scripts, both modern and historical, which people are still working to add.  Given this enormous diversity, it’s inevitable that representing it is a complicated project. Unicode embraces that diversity, and accepts the complexity inherent in its mission to include all human writing systems. It doesn’t make a lot of trade-offs in the name of simplification, and it makes exceptions to its own rules where necessary to further its mission.  Moreover, Unicode is committed not just to supporting texts in any single language, but also to letting multiple languages coexist within one text—which introduces even more complexity.  Most programming languages have libraries available to handle the gory low-level details of text manipulation, but as a programmer, you’ll still need to know about certain Unicode features in order to know when and how to apply them. It may take some time to wrap your head around it all, but don’t be discouraged—think about the billions of people for whom your software will be more accessible through supporting text in their language. Embrace the complexity!  The Unicode Codespace Let’s start with some general orientation. The basic elements of Unicode—its “characters”, although that term isn’t quite right—are called code points. Code points are identified by number, customarily written in hexadecimal with the prefix “U+”, such as U+0041 “A” latin capital letter a or U+03B8 “θ” greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database.  The set of all possible code points is called the codespace. The Unicode codespace consists of 1,114,112 code points. However, only 128,237 of them—about 12% of the codespace—are actually assigned, to date. There’s plenty of room for growth! Unicode also reserves an additional 137,468 code points as “private use” areas, which have no standardized meaning and are available for individual applications to define for their own purposes.  Codespace Allocation To get a feel for how the codespace is laid out, it’s helpful to visualize it. Below is a map of the entire codespace, with one pixel per code point. It’s arranged in tiles for visual coherence; each small square is 16×16 = 256 code points, and each large square is a “plane” of 65,536 code points. There are 17 planes altogether.  Map of the Unicode codespace (click to zoom)  White represents unassigned space. Blue is assigned code points, green is private-use areas, and the small red area is surrogates (more about those later). As you can see, the assigned code points are distributed somewhat sparsely, but concentrated in the first three planes.  Plane 0 is also known as the “Basic Multilingual Plane”, or BMP. The BMP contains essentially all the characters needed for modern text in any script, including Latin, Cyrillic, Greek, Han (Chinese), Japanese, Korean, Arabic, Hebrew, Devanagari (Indian), and many more.  (In the past, the codespace was just the BMP and no more—Unicode was originally conceived as a straightforward 16-bit encoding, with only 65,536 code points. It was expanded to its current size in 1996. However, the vast majority of code points in modern text belong to the BMP.)  Plane 1 contains historical scripts, such as Sumerian cuneiform and Egyptian hieroglyphs, as well as emoji and various other symbols. Plane 2 contains a large block of less-common and historical Han characters. The remaining planes are empty, except for a small number of rarely-used formatting characters in Plane 14; planes 15–16 are reserved entirely for private use.  Scripts Let’s zoom in on the first three planes, since that’s where the action is:  Map of scripts in Unicode planes 0–2 (click to zoom)  This map color-codes the 135 different scripts in Unicode. You can see how Han () and Korean () take up most of the range of the BMP (the left large square). By contrast, all of the European, Middle Eastern, and South Asian scripts fit into the first row of the BMP in this diagram.  Many areas of the codespace are adapted or copied from earlier encodings. For example, the first 128 code points of Unicode are just a copy of ASCII. This has clear benefits for compatibility—it’s easy to losslessly convert texts from smaller encodings into Unicode (and the other direction too, as long as no characters outside the smaller encoding are used).  Usage Frequency One more interesting way to visualize the codespace is to look at the distribution of usage—in other words, how often each code point is actually used in real-world texts. Below is a heat map of planes 0–2 based on a large sample of text from Wikipedia and Twitter (all languages). Frequency increases from black (never seen) through red and yellow to white.  Heat map of code point usage frequency in Unicode planes 0–2 (click to zoom)  You can see that the vast majority of this text sample lies in the BMP, with only scattered usage of code points from planes 1–2. The biggest exception is emoji, which show up here as the several bright squares in the bottom row of plane 1.  Encodings We’ve seen that Unicode code points are abstractly identified by their index in the codespace, ranging from U+0000 to U+10FFFF. But how do code points get represented as bytes, in memory or in a file?  The most convenient, computer-friendliest (and programmer-friendliest) thing to do would be to just store the code point index as a 32-bit integer. This works, but it consumes 4 bytes per code point, which is sort of a lot. Using 32-bit ints for Unicode will cost you a bunch of extra storage, memory, and performance in bandwidth-bound scenarios, if you work with a lot of text.  Consequently, there are several more-compact encodings for Unicode. The 32-bit integer encoding is officially called UTF-32 (UTF = “Unicode Transformation Format”), but it’s rarely used for storage. At most, it comes up sometimes as a temporary internal representation, for examining or operating on the code points in a string.  Much more commonly, you’ll see Unicode text encoded as either UTF-8 or UTF-16. These are both variable-length encodings, made up of 8-bit or 16-bit units, respectively. In these schemes, code points with smaller index values take up fewer bytes, which saves a lot of memory for typical texts. The trade-off is that processing UTF-8/16 texts is more programmatically involved, and likely slower.  UTF-8 In UTF-8, each code point is stored using 1 to 4 bytes, based on its index value.  UTF-8 uses a system of binary prefixes, in which the high bits of each byte mark whether it’s a single byte, the beginning of a multi-byte sequence, or a continuation byte; the remaining bits, concatenated, give the code point index. This table shows how it works:  UTF-8 (binary)\\tCode point (binary)\\tRange 0xxxxxxx\\txxxxxxx\\tU+0000–U+007F 110xxxxx 10yyyyyy\\txxxxxyyyyyy\\tU+0080–U+07FF 1110xxxx 10yyyyyy 10zzzzzz\\txxxxyyyyyyzzzzzz\\tU+0800–U+FFFF 11110xxx 10yyyyyy 10zzzzzz 10wwwwww\\txxxyyyyyyzzzzzzwwwwww\\tU+10000–U+10FFFF A handy property of UTF-8 is that code points below 128 (ASCII characters) are encoded as single bytes, and all non-ASCII code points are encoded using sequences of bytes 128–255. This has a couple of nice consequences. First, any strings or files out there that are already in ASCII can also be interpreted as UTF-8 without any conversion. Second, lots of widely-used string programming idioms—such as null termination, or delimiters (newlines, tabs, commas, slashes, etc.)—will just work on UTF-8 strings. ASCII bytes never occur inside the encoding of non-ASCII code points, so searching byte-wise for a null terminator or a delimiter will do the right thing.  Thanks to this convenience, it’s relatively simple to extend legacy ASCII programs and APIs to handle UTF-8 strings. UTF-8 is very widely used in the Unix/Linux and Web worlds, and many programmers argue UTF-8 should be the default encoding everywhere.  However, UTF-8 isn’t a drop-in replacement for ASCII strings in all respects. For instance, code that iterates over the “characters” in a string will need to decode UTF-8 and iterate over code points (or maybe grapheme clusters—more about those later), not bytes. When you measure the “length” of a string, you’ll need to think about whether you want the length in bytes, the length in code points, the width of the text when rendered, or something else.  UTF-16 The other encoding that you’re likely to encounter is UTF-16. It uses 16-bit words, with each code point stored as either 1 or 2 words.  Like UTF-8, we can express the UTF-16 encoding rules in the form of binary prefixes:  UTF-16 (binary)\\tCode point (binary)\\tRange xxxxxxxxxxxxxxxx\\txxxxxxxxxxxxxxxx\\tU+0000–U+FFFF 110110xxxxxxxxxx 110111yyyyyyyyyy\\txxxxxxxxxxyyyyyyyyyy + 0x10000\\tU+10000–U+10FFFF A more common way that people talk about UTF-16 encoding, though, is in terms of code points called “surrogates”. All the code points in the range U+D800–U+DFFF—or in other words, the code points that match the binary prefixes 110110 and 110111 in the table above—are reserved specifically for UTF-16 encoding, and don’t represent any valid characters on their own. They’re only meant to occur in the 2-word encoding pattern above, which is called a “surrogate pair”. Surrogate code points are illegal in any other context! They’re not allowed in UTF-8 or UTF-32 at all.  Historically, UTF-16 is a descendant of the original, pre-1996 versions of Unicode, in which there were only 65,536 code points. The original intention was that there would be no different “encodings”; Unicode was supposed to be a straightforward 16-bit character set. Later, the codespace was expanded to make room for a long tail of less-common (but still important) Han characters, which the Unicode designers didn’t originally plan for. Surrogates were then introduced, as—to put it bluntly—a kludge, allowing 16-bit encodings to access the new code points.  Today, Javascript uses UTF-16 as its standard string representation: if you ask for the length of a string, or iterate over it, etc., the result will be in UTF-16 words, with any code points outside the BMP expressed as surrogate pairs. UTF-16 is also used by the Microsoft Win32 APIs; though Win32 supports either 8-bit or 16-bit strings, the 8-bit version unaccountably still doesn’t support UTF-8—only legacy code-page encodings, like ANSI. This leaves UTF-16 as the only way to get proper Unicode support in Windows. (Update: in Win10 version 1903, they finally added UTF-8 support to the 8-bit APIs! 😊)  By the way, UTF-16’s words can be stored either little-endian or big-endian. Unicode has no opinion on that issue, though it does encourage the convention of putting U+FEFF zero width no-break space at the top of a UTF-16 file as a byte-order mark, to disambiguate the endianness. (If the file doesn’t match the system’s endianness, the BOM will be decoded as U+FFFE, which isn’t a valid code point.)  Combining Marks In the story so far, we’ve been focusing on code points. But in Unicode, a “character” can be more complicated than just an individual code point!  Unicode includes a system for dynamically composing characters, by combining multiple code points together. This is used in various ways to gain flexibility without causing a huge combinatorial explosion in the number of code points.  In European languages, for example, this shows up in the application of diacritics to letters. Unicode supports a wide range of diacritics, including acute and grave accents, umlauts, cedillas, and many more. All these diacritics can be applied to any letter of any alphabet—and in fact, multiple diacritics can be used on a single letter.  If Unicode tried to assign a distinct code point to every possible combination of letter and diacritics, things would rapidly get out of hand. Instead, the dynamic composition system enables you to construct the character you want, by starting with a base code point (the letter) and appending additional code points, called “combining marks”, to specify the diacritics. When a text renderer sees a sequence like this in a string, it automatically stacks the diacritics over or under the base letter to create a composed character.  For example, the accented character “Á” can be expressed as a string of two code points: U+0041 “A” latin capital letter a plus U+0301 “◌́” combining acute accent. This string automatically gets rendered as a single character: “Á”.  Now, Unicode does also include many “precomposed” code points, each representing a letter with some combination of diacritics already applied, such as U+00C1 “Á” latin capital letter a with acute or U+1EC7 “ệ” latin small letter e with circumflex and dot below. I suspect these are mostly inherited from older encodings that were assimilated into Unicode, and kept around for compatibility. In practice, there are precomposed code points for most of the common letter-with-diacritic combinations in European-script languages, so they don’t use dynamic composition that much in typical text.  Still, the system of combining marks does allow for an arbitrary number of diacritics to be stacked on any base character. The reductio-ad-absurdum of this is Zalgo text, which works by ͖͟ͅr͞aṋ̫̠̖͈̗d͖̻̹óm̪͙͕̗̝ļ͇̰͓̳̫ý͓̥̟͍ ̕s̫t̫̱͕̗̰̼̘͜a̼̩͖͇̠͈̣͝c̙͍k̖̱̹͍͘i̢n̨̺̝͇͇̟͙ģ̫̮͎̻̟ͅ ̕n̼̺͈͞u̮͙m̺̭̟̗͞e̞͓̰̤͓̫r̵o̖ṷs҉̪͍̭̬̝̤ ̮͉̝̞̗̟͠d̴̟̜̱͕͚i͇̫̼̯̭̜͡ḁ͙̻̼c̲̲̹r̨̠̹̣̰̦i̱t̤̻̤͍͙̘̕i̵̜̭̤̱͎c̵s ͘o̱̲͈̙͖͇̲͢n͘ ̜͈e̬̲̠̩ac͕̺̠͉h̷̪ ̺̣͖̱ḻ̫̬̝̹ḙ̙̺͙̭͓̲t̞̞͇̲͉͍t̷͔̪͉̲̻̠͙e̦̻͈͉͇r͇̭̭̬͖,̖́ ̜͙͓̣̭s̘̘͈o̱̰̤̲ͅ ̛̬̜̙t̼̦͕̱̹͕̥h̳̲͈͝ͅa̦t̻̲ ̻̟̭̦̖t̛̰̩h̠͕̳̝̫͕e͈̤̘͖̞͘y҉̝͙ ̷͉͔̰̠o̞̰v͈͈̳̘͜er̶f̰͈͔ḻ͕̘̫̺̲o̲̭͙͠ͅw̱̳̺ ͜t̸h͇̭͕̳͍e̖̯̟̠ ͍̞̜͔̩̪͜ļ͎̪̲͚i̝̲̹̙̩̹n̨̦̩̖ḙ̼̲̼͢ͅ ̬͝s̼͚̘̞͝p͙̘̻a̙c҉͉̜̤͈̯̖i̥͡n̦̠̱͟g̸̗̻̦̭̮̟ͅ ̳̪̠͖̳̯̕a̫͜n͝d͡ ̣̦̙ͅc̪̗r̴͙̮̦̹̳e͇͚̞͔̹̫͟a̙̺̙ț͔͎̘̹ͅe̥̩͍ a͖̪̜̮͙̹n̢͉̝ ͇͉͓̦̼́a̳͖̪̤̱p̖͔͔̟͇͎͠p̱͍̺ę̲͎͈̰̲̤̫a̯͜r̨̮̫̣̘a̩̯͖n̹̦̰͎̣̞̞c̨̦̱͔͎͍͖e̬͓͘ ̤̰̩͙̤̬͙o̵̼̻̬̻͇̮̪f̴ ̡̙̭͓͖̪̤“̸͙̠̼c̳̗͜o͏̼͙͔̮r̞̫̺̞̥̬ru̺̻̯͉̭̻̯p̰̥͓̣̫̙̤͢t̳͍̳̖ͅi̶͈̝͙̼̙̹o̡͔n̙̺̹̖̩͝ͅ”̨̗͖͚̩.̯͓  A few other places where dynamic character composition shows up in Unicode:  Vowel-pointing notation in Arabic and Hebrew. In these languages, words are normally spelled with some of their vowels left out. They then have diacritic notation to indicate the vowels (used in dictionaries, language-teaching materials, children’s books, and such). These diacritics are expressed with combining marks.  A Hebrew example, with niqqud:\\tאֶת דַלְתִּי הֵזִיז הֵנִיעַ, קֶטֶב לִשְׁכַּתִּי יָשׁוֹד Normal writing (no niqqud):\\tאת דלתי הזיז הניע, קטב לשכתי ישוד Devanagari, the script used to write Hindi, Sanskrit, and many other South Asian languages, expresses certain vowels as combining marks attached to consonant letters. For example, “ह” + “\\u200bि” = “हि” (“h” + “i” = “hi”). Korean characters stand for syllables, but they are composed of letters called jamo that stand for the vowels and consonants in the syllable. While there are code points for precomposed Korean syllables, it’s also possible to dynamically compose them by concatenating their jamo. For example, “ᄒ” + “ᅡ” + “ᆫ” = “한” (“h” + “a” + “n” = “han”). Canonical Equivalence In Unicode, precomposed characters exist alongside the dynamic composition system. A consequence of this is that there are multiple ways to express “the same” string—different sequences of code points that result in the same user-perceived characters. For example, as we saw earlier, we can express the character “Á” either as the single code point U+00C1, or as the string of two code points U+0041 U+0301.  Another source of ambiguity is the ordering of multiple diacritics in a single character. Diacritic order matters visually when two diacritics apply to the same side of the base character, e.g. both above: “ǡ” (dot, then macron) is different from “ā̇” (macron, then dot). However, when diacritics apply to different sides of the character, e.g. one above and one below, then the order doesn’t affect rendering. Moreover, a character with multiple diacritics might have one of the diacritics precomposed and others expressed as combining marks.  For example, the Vietnamese letter “ệ” can be expressed in five different ways:  Fully precomposed: U+1EC7 “ệ” Partially precomposed: U+1EB9 “ẹ” + U+0302 “◌̂” Partially precomposed: U+00EA “ê” + U+0323 “◌̣” Fully decomposed: U+0065 “e” + U+0323 “◌̣” + U+0302 “◌̂” Fully decomposed: U+0065 “e” + U+0302 “◌̂” + U+0323 “◌̣” Unicode refers to set of strings like this as “canonically equivalent”. Canonically equivalent strings are supposed to be treated as identical for purposes of searching, sorting, rendering, text selection, and so on. This has implications for how you implement operations on text. For example, if an app has a “find in file” operation and the user searches for “ệ”, it should, by default, find occurrences of any of the five versions of “ệ” above!  Normalization Forms To address the problem of “how to handle canonically equivalent strings”, Unicode defines several normalization forms: ways of converting strings into a canonical form so that they can be compared code-point-by-code-point (or byte-by-byte).  The “NFD” normalization form fully decomposes every character down to its component base and combining marks, taking apart any precomposed code points in the string. It also sorts the combining marks in each character according to their rendered position, so e.g. diacritics that go below the character come before the ones that go above the character. (It doesn’t reorder diacritics in the same rendered position, since their order matters visually, as previously mentioned.)  The “NFC” form, conversely, puts things back together into precomposed code points as much as possible. If an unusual combination of diacritics is called for, there may not be any precomposed code point for it, in which case NFC still precomposes what it can and leaves any remaining combining marks in place (again ordered by rendered position, as in NFD).  There are also forms called NFKD and NFKC. The “K” here refers to compatibility decompositions, which cover characters that are “similar” in some sense but not visually identical. However, I’m not going to cover that here.  Grapheme Clusters As we’ve seen, Unicode contains various cases where a thing that a user thinks of as a single “character” might actually be made up of multiple code points under the hood. Unicode formalizes this using the notion of a grapheme cluster: a string of one or more code points that constitute a single “user-perceived character”.  UAX #29 defines the rules for what, precisely, qualifies as a grapheme cluster. It’s approximately “a base code point followed by any number of combining marks”, but the actual definition is a bit more complicated; it accounts for things like Korean jamo, and emoji ZWJ sequences.  The main thing grapheme clusters are used for is text editing: they’re often the most sensible unit for cursor placement and text selection boundaries. Using grapheme clusters for these purposes ensures that you can’t accidentally chop off some diacritics when you copy-and-paste text, that left/right arrow keys always move the cursor by one visible character, and so on.  Another place where grapheme clusters are useful is in enforcing a string length limit—say, on a database field. While the true, underlying limit might be something like the byte length of the string in UTF-8, you wouldn’t want to enforce that by just truncating bytes. At a minimum, you’d want to “round down” to the nearest code point boundary; but even better, round down to the nearest grapheme cluster boundary. Otherwise, you might be corrupting the last character by cutting off a diacritic, or interrupting a jamo sequence or ZWJ sequence.  And More… There’s much more that could be said about Unicode from a programmer’s perspective! I haven’t gotten into such fun topics as case mapping, collation, compatibility decompositions and confusables, Unicode-aware regexes, or bidirectional text. Nor have I said anything yet about implementation issues—how to efficiently store and look-up data about the sparsely-assigned code points, or how to optimize UTF-8 decoding, string comparison, or NFC normalization. Perhaps I’ll return to some of those things in future posts.  Unicode is a fascinating and complex system. It has a many-to-one mapping between bytes and code points, and on top of that a many-to-one (or, under some circumstances, many-to-many) mapping between code points and “characters”. It has oddball special cases in every corner. But no one ever claimed that representing all written languages was going to be easy, and it’s clear that we’re never going back to the bad old days of a patchwork of incompatible encodings.  Further reading:  The Unicode Standard UTF-8 Everywhere Manifesto Dark corners of Unicode by Eevee ICU (International Components for Unicode)—C/C++/Java libraries implementing many Unicode algorithms and related things Python 3 Unicode Howto Google Noto Fonts—set of fonts intended to cover all assigned code points\"\"\"\n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f35a6fe-5701-4b9c-b20e-097b8bd0b161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging pair (101, 32) into new token 256\n",
      "Merging pair (105, 110) into new token 257\n",
      "Merging pair (115, 32) into new token 258\n",
      "Merging pair (116, 104) into new token 259\n",
      "Merging pair (101, 114) into new token 260\n",
      "Merging pair (99, 111) into new token 261\n",
      "Merging pair (116, 32) into new token 262\n",
      "Merging pair (226, 128) into new token 263\n",
      "Merging pair (44, 32) into new token 264\n",
      "Merging pair (97, 110) into new token 265\n",
      "Merging pair (111, 114) into new token 266\n",
      "Merging pair (100, 32) into new token 267\n",
      "Merging pair (97, 114) into new token 268\n",
      "Merging pair (101, 110) into new token 269\n",
      "Merging pair (257, 103) into new token 270\n",
      "Merging pair (261, 100) into new token 271\n",
      "Merging pair (121, 32) into new token 272\n",
      "Merging pair (46, 32) into new token 273\n",
      "Merging pair (97, 108) into new token 274\n",
      "Merging pair (259, 256) into new token 275\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): \n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "\n",
    "# ---\n",
    "\n",
    "vocab_size = 276 # desired final vocab_size\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens)\n",
    "\n",
    "merges = {} # (int, int) --> int\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    print(f\"Merging pair {pair} into new token {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "faf43ea5-02a1-4d27-afd8-27688065c8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length:  24597\n",
      "ids length:  19438\n",
      "compression ratio: 1.27X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length: \", len(tokens))\n",
    "print(\"ids length: \", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743771df-9f06-4414-ac1f-29b53a4b3d7f",
   "metadata": {},
   "source": [
    "---\n",
    "Note, the Tokenizer is a completely separate, independent module from the LLM. It has its own training dataset of text (which could be different from that of the LLM), on which you train the vocabulary using the Byte Pair Encoding (BPE) algorithm. It then translates back and forth between raw text and sequences of tokens. The LLM later only ever sees the tokens and never directly deals with any text."
   ]
  },
  {
   "attachments": {
    "116846a3-b4c5-4f85-9e2d-75fd32e9630f.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "8b94e578-179f-4d55-ab55-a49300b77df0",
   "metadata": {},
   "source": [
    "![image.png](attachment:116846a3-b4c5-4f85-9e2d-75fd32e9630f.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b517525f-2641-49fd-931c-2d4759109d38",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "Given a sequence of integers in the range \\[0, vocab_size], what is the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1d4b437c-2300-4cc8-a22c-3befbafd1fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "\n",
    "def decode(ids):\n",
    "    # given ids (list of integers), return Python string\n",
    "    tokens = b\"\".join(vocab[x] for x in ids)\n",
    "    text = tokens.decode('utf-8', errors=\"replace\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603bf115-a66d-43e8-ac5d-a52dcf4f123b",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "The other way - given a string, what are the tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2eae2faa-1b2a-4248-982c-58d6ef0770e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    # given a string, return a list of integers\n",
    "    tokens = list(text.encode('utf-8'))\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_stats(tokens)\n",
    "        pair = min(stats, key = lambda p: merges.get(p, float('inf')))\n",
    "        if pair not in merges:\n",
    "            break\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "df8223e0-4c40-4895-bdcc-90ae103aa6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2e487e75-42fd-4596-ad37-f25c75e60fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = decode(encode(text))\n",
    "text == text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "32aecabb-c3ca-48c6-b327-d3265a8691cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_text = \"SolidGoldMagikarp (plus, prompt generation)by Jessica Rumbelow, mwatkins5th Feb 2023AI Alignment ForumUPDATE (14th Feb 2023): ChatGPT appears to have been patched! However, very strange behaviour can still be elicited in the OpenAI playground, particularly with the davinci-instruct model.More technical details here.Further (fun) investigation into the stories behind the tokens we found here. Work done at SERI-MATS, over the past two months, by Jessica Rumbelow and Matthew Watkins.TL;DRAnomalous tokens: a mysterious failure mode for GPT (which reliably insulted Matthew)We have found a set of anomalous tokens which result in a previously undocumented failure mode for GPT-2 and GPT-3 models. (The \\'instruct\\' models “are particularly deranged” in this context, as janus has observed.)Many of these tokens reliably break determinism in the OpenAI GPT-3 playground at temperature 0 (which theoretically shouldn\\'t happen).Prompt generation: a new interpretability method for language models (which reliably finds prompts that result in a target completion). This is good for:eliciting knowledgegenerating adversarial inputsautomating prompt search (e.g. for fine-tuning)In this post, we\\'ll introduce the prototype of a new model-agnostic interpretability method for language models which reliably generates adversarial prompts that result in a target completion. We\\'ll also demonstrate a previously undocumented failure mode for GPT-2 and GPT-3 language models, which results in bizarre completions (in some cases explicitly contrary to the purpose of the model), and present the results of our investigation into this phenomenon. Further technical detail can be found in a follow-up post. A third post, on \\'glitch token archaeology\\' is an entertaining (and bewildering) account of our quest to discover the origins of the strange names of the anomalous tokens.A rather unexpected prompt completion from the GPT-3 davinci-instruct-beta model.Prompt generationFirst up, prompt generation. An easy intuition for this is to think about feature visualisation for image classifiers (an excellent explanation here, if you\\'re unfamiliar with the concept).Feature visualisation of VGG network by Tim Sainburg.We can study how a neural network represents concepts by taking some random input and using gradient descent to tweak it until it it maximises a particular activation. The image above shows the resulting inputs that maximise the output logits for the classes \\'goldfish\\', \\'monarch\\', \\'tarantula\\' and \\'flamingo\\'. This is pretty cool! We can see what VGG thinks is the most \\'goldfish\\'-y thing in the world, and it\\'s got scales and fins. Note though, that it isn\\'t a picture of a single goldfish. We\\'re not seeing the kind of input that VGG was trained on. We\\'re seeing what VGG has learned. This is handy: if you wanted to sanity check your goldfish detector, and the feature visualisation showed just water, you\\'d know that the model hadn\\'t actually learned to detect goldfish, but rather the environments in which they typically appear. So it would label every image containing water as \\'goldfish\\', which is probably not what you want. Time to go get some more training data.So, how can we apply this approach to language models?GPT2-xl optimised inputs to maximise (boldface) outputsSome interesting stuff here. Note that as with image models, we\\'re not optimising for realistic inputs, but rather for inputs that maximise the output probability of the target completion, shown in bold above.So now we can do stuff like this:Comparing \\'sensible\\' prompts (i.e. ones that we wrote) with generated prompts (in bold) to maximise probability of target completion. The model used was GPT-2 small.And this:The result of optimising a prompt to maximise a target token many times with different random seeds, then aggregating token frequencies.We\\'ll leave it to you to lament the state of the internet that results in the above optimised inputs for the token \\' girl\\'.How do we do this? It\\'s tricky, because unlike pixel values, the inputs to LLMs are discrete tokens. This is not conducive to gradient descent. However, these discrete tokens are mapped to embeddings, which do occupy a continuous space, albeit sparsely. (Most of this space doesn\\'t correspond actual tokens – there is a lot of space between tokens in embedding space, and we don\\'t want to find a solution there.) However, with a combination of regularisation and explicit coercion to keep embeddings close to the realm of legal tokens during optimisation, we can make it work. Code available here if you want more detail.This kind of prompt generation is only possible because token embedding space has a kind of semantic coherence. Semantically related tokens tend to be found close together. We discovered this by carrying out k-means clustering over the embedding space of the GPT token set, and found many clusters that are surprisingly robust to random initialisation of the centroids. Here are a few examples:Clustering tokens in embedding space. Here we see the five tokens from each of a few random clusters. But what\\'s going on in that right-most cluster?Finding weird tokensDuring this process we found some weird looking tokens. Here’s how that happened. We were interested in the semantic relevance of the clusters produced by the k-means algorithm, and in order to probe this, we looked for the nearest legal token embedding to the centroid of each cluster. However, something seemed to be wrong, because the tokens looked strange and didn\\'t seem semantically relevant to the cluster (or anything else). And over many runs we kept seeing the same handful of tokens playing this role, all  very “untokenlike” in their appearance. There were what appeared to be some special characters and control characters, but also long, unfamiliar strings like \\' TheNitromeFan\\', \\' SolidGoldMagikarp\\' and \\'cloneembedreportprint\\'. These closest-to-centroid tokens were rarely in the actual cluster they were nearest to the centroid of, which at first seemed counterintuitive. Such is the nature of 768-dimensional space, we tentatively reasoned! The puzzling tokens seemed to have a tendency to aggregate together into a few clusters of their own.We pursued a hypothesis that perhaps these were the closest tokens to the origin of the embedding space, i.e. those with the smallest norm[1]. That turned out to be wrong. But a revised hypothesis, that many of these tokens we were seeing were among those closest to the centroid of the entire set of 50,257 tokens, turned out to be correct.  This centroid can be imagined as the centre-of-mass of the whole “cloud” of tokens in embedding space. Here are the 50 closest-to-centroid tokens for the GPT-J model[2]:Token:\"\n",
    "val_text2 = decode(encode(val_text))\n",
    "val_text == val_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "deea9cce-e703-4b07-98ed-be61954b2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "del val_text, val_text2, text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e4e354dc-6ce2-4b6f-9e82-42ab031619da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' dog', ' can', \"'t\", ' but', ' shouldn', \"'t\"]\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(pat, \"Hello dog can't but shouldn't\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9b7301bb-8f42-4c89-9143-bb022fd498f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 220, 23748, 995, 10185]\n",
      "[257, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "print(enc.encode(\"     hello world!!!\"))\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding('cl100k_base')\n",
    "print(enc.encode(\"     hello world!!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "46b7e94a-989a-461e-bdfb-86dc84c926c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved under vocab (1).bpe\n",
      "\n",
      "Saved under encoder.json\n"
     ]
    }
   ],
   "source": [
    "!python -m wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
    "!python -m wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "1bb510ee-f7ed-4e4c-bd26-14323a5b02ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "with open('encoder.json', 'r') as f:\n",
    "    encoder = json.load(f) # <--- ~equivalent to our \"vocab\"\n",
    "\n",
    "with open('vocab.bpe', 'r', encoding=\"utf-8\") as f:\n",
    "    bpe_data = f.read()\n",
    "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "# ^---- ~equivalent to our \"merges\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f44dcde-2d58-4fcb-9911-e4bcccf4516c",
   "metadata": {},
   "source": [
    "### Special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a53e82b4-3280-484b-9032-0a8785435eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder) # 256 raw byte tokens, 50 000 merges + 1 special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e60a2ec9-4547-4f92-af64-118e2458cfce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder['<|endoftext|>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39202c5f-cefe-4b6b-9668-f8802e319911",
   "metadata": {},
   "source": [
    "### sentencepiece\n",
    "\n",
    "Commonly used because (unlike tiktoken) it can efficiently both train and inference BPE tokenizers. It is used in both Llama and Mistral series.\n",
    "\n",
    "[sentencepiece on Github link](https://github.com/google/sentencepiece).\n",
    "\n",
    "**The big difference**: sentencepiece runs BPE on the Unicode code points directly! It then has an option `character_coverage` for what to do with very very rare codepoints that appear very few times, and it either maps them onto an UNK token, or if `byte_fallback` is turned on, it encodes them with utf-8 and then encodes the raw bytes instead.\n",
    "\n",
    "TLDR:\n",
    "\n",
    "- tiktoken encodes to utf-8 and then BPEs bytes\n",
    "- sentencepiece BPEs the code points and optionally falls back to utf-8 bytes for rare code points (rarity is determined by character_coverage hyperparameter), which then get translated to byte tokens.\n",
    "\n",
    "(Personally I think the tiktoken way is a lot cleaner...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b7c15c92-1e90-4855-bbe1-4507df654864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2f8ff249-fa47-47b7-8a03-1af68f901800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a toy.txt file with some random text\n",
    "with open(\"toy.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "  f.write(\"SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3af74b7-0dba-4607-aec3-293ec0df7159",
   "metadata": {},
   "source": [
    "Docs for sentencepiece options:\n",
    "\n",
    "- [markdown](https://github.com/google/sentencepiece/blob/master/doc/options.md)\n",
    "- [protobuf](https://github.com/google/sentencepiece/blob/master/src/sentencepiece_model.proto#L193)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "926561ba-c390-4886-9a70-39136143b8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a sentencepiece model on it\n",
    "# the settings here are (best effort) those used for training Llama 2\n",
    "import os\n",
    "\n",
    "options = dict(\n",
    "  # input spec\n",
    "  input=\"toy.txt\",\n",
    "  input_format=\"text\",\n",
    "  # output spec\n",
    "  model_prefix=\"tok400\", # output filename prefix\n",
    "  # algorithm spec\n",
    "  # BPE alg\n",
    "  model_type=\"bpe\",\n",
    "  vocab_size=400,\n",
    "  # normalization\n",
    "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
    "  remove_extra_whitespaces=False,\n",
    "  input_sentence_size=200000000, # max number of training sentences\n",
    "  max_sentence_length=4192, # max number of bytes per sentence\n",
    "  seed_sentencepiece_size=1000000,\n",
    "  shuffle_input_sentence=True,\n",
    "  # rare word treatment\n",
    "  character_coverage=0.99995,\n",
    "                                  byte_fallback=False,#True, \n",
    "  # merge rules\n",
    "  split_digits=True,\n",
    "  split_by_unicode_script=True,\n",
    "  split_by_whitespace=True,\n",
    "  split_by_number=True,\n",
    "  max_sentencepiece_length=16,\n",
    "  add_dummy_prefix=True,\n",
    "  allow_whitespace_only_pieces=True,\n",
    "  # special tokens\n",
    "  unk_id=0, # the UNK token MUST exist\n",
    "  bos_id=1, # the others are optional, set to -1 to turn off\n",
    "  eos_id=2,\n",
    "  pad_id=-1,\n",
    "  # systems\n",
    "  num_threads=os.cpu_count(), # use ~all system resources\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(**options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98567c28-f679-43f3-82e7-09beca54d3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('tok400.model')\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "573986f3-87a3-4ce8-8ee6-863a74ef02b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[362, 378, 252, 102, 362, 0]\n"
     ]
    }
   ],
   "source": [
    "ids = sp.encode(\"hello 안녕하세요\")\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e91dd66d-0149-4e88-9bae-d40a4532a71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'h', 'el', 'lo', '▁', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "print([sp.id_to_piece(idx) for idx in ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1674fbe-99ef-4f03-9356-8b377c5b7701",
   "metadata": {},
   "source": [
    "**Llama 2 tokenizer proto**\n",
    "If you'd like to export the raw protocol buffer for the `tokenizer.model` released by meta, this is a [helpful issue](https://github.com/google/sentencepiece/issues/121). And this is the result:\n",
    "\n",
    "```\n",
    "normalizer_spec {\n",
    "  name: \"identity\"\n",
    "  precompiled_charsmap: \"\"\n",
    "  add_dummy_prefix: true\n",
    "  remove_extra_whitespaces: false\n",
    "  normalization_rule_tsv: \"\"\n",
    "}\n",
    "\n",
    "trainer_spec {\n",
    "  input: \"/large_experiments/theorem/datasets/MERGED/all.test1.merged\"\n",
    "  model_prefix: \"spm_model_32k_200M_charcov099995_allowWSO__v2\"\n",
    "  model_type: BPE\n",
    "  vocab_size: 32000\n",
    "  self_test_sample_size: 0\n",
    "  input_format: \"text\"\n",
    "  character_coverage: 0.99995\n",
    "  input_sentence_size: 200000000\n",
    "  seed_sentencepiece_size: 1000000\n",
    "  shrinking_factor: 0.75\n",
    "  num_threads: 80\n",
    "  num_sub_iterations: 2\n",
    "  max_sentence_length: 4192\n",
    "  shuffle_input_sentence: true\n",
    "  max_sentencepiece_length: 16\n",
    "  split_by_unicode_script: true\n",
    "  split_by_whitespace: true\n",
    "  split_by_number: true\n",
    "  treat_whitespace_as_suffix: false\n",
    "  split_digits: true\n",
    "  allow_whitespace_only_pieces: true\n",
    "  vocabulary_output_piece_score: true\n",
    "  hard_vocab_limit: true\n",
    "  use_all_vocab: false\n",
    "  byte_fallback: true\n",
    "  required_chars: \"\"\n",
    "  unk_id: 0\n",
    "  bos_id: 1\n",
    "  eos_id: 2\n",
    "  pad_id: -1\n",
    "  unk_surface: \" \\342\\201\\207 \"\n",
    "  unk_piece: \"<unk>\"\n",
    "  bos_piece: \"<s>\"\n",
    "  eos_piece: \"</s>\"\n",
    "  pad_piece: \"<pad>\"\n",
    "  train_extremely_large_corpus: false\n",
    "  enable_differential_privacy: false\n",
    "  differential_privacy_noise_level: 0.0\n",
    "  differential_privacy_clipping_threshold: 0\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a2e482-52b8-4b29-a675-37e1d5aa6581",
   "metadata": {},
   "source": [
    "#### vocab_size\n",
    "\n",
    "- Q: what should be vocab size?\n",
    "- Q: how can I increase vocab size?\n",
    "- A: let's see. Reminder: [gpt.py](https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py) from before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d7a4e-dd7a-4b9a-8b61-048527523f74",
   "metadata": {},
   "source": [
    "### Final recommendations\n",
    "\n",
    "- Don't brush off tokenization. A lot of footguns and sharp edges here. Security issues. Safety issues.\n",
    "- Eternal glory to anyone who can delete tokenization as a required step in LLMs.\n",
    "- In your own application:\n",
    "  - Maybe you can just re-use the GPT-4 tokens and tiktoken?\n",
    "  - If you're training a vocab, ok to use BPE with sentencepiece. Careful with the million settings.\n",
    "  - Switch to minbpe once it is as efficient as sentencepiece :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dbc874-adca-4804-bf7a-2d0c249425d2",
   "metadata": {},
   "source": [
    "### Also worth looking at\n",
    "\n",
    "- [Huggingface Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer). I didn't cover it in detail in the lecture because the algorithm (to my knowledge) is very similar to sentencepiece, but worth potentially evaluating for use in practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workproject",
   "language": "python",
   "name": "workproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
