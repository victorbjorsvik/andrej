import os


!ls


words = open("names/girl_names.txt", "r").read().splitlines()


words = [word.lower() for word in words]


words[:10]


len(words)


min(len(w) for w in words)


max(len(w) for w in words)


b = {}
for w in words:
    chs = ["<S>"] + list(w) + ["<E>"]
    for ch1, ch2 in  zip(chs, chs[1:]):
        bigram = (ch1, ch2)
        b[bigram] = b.get(bigram, 0) + 1


sorted(b.items(), key = lambda kv: -kv[1])


import torch


N = torch.zeros((33,33), dtype=torch.int32)


chars = sorted(list(set("".join(words))))
stoi = {s:i+1 for i,s in enumerate(chars)}
stoi["."] = 0
itos = {i:s for s,i in stoi.items()}


for w in words:
    chs = ["."] + list(w) + ["."]
    for ch1, ch2 in  zip(chs, chs[1:]):
        ix1 = stoi[ch1]
        ix2 = stoi[ch2]
        N[ix1, ix2] +=1 


import matplotlib.pyplot as plt
%matplotlib inline

plt.figure(figsize=(16,16))
plt.imshow(N, cmap="Blues")
for i in range(33):
    for j in range(33):
        chstr = itos[i] + itos[j]
        plt.text(j, i, chstr, ha="center", va="bottom", color="gray")
        plt.text(j, i, N[i,j].item(), ha="center", va="top", color="gray")
plt.axis("off");


N[0]


p = N[0].float()
p = p / p.sum()
p


g = torch.Generator().manual_seed(2147483647)
ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
itos[ix]


g = torch.Generator().manual_seed(2147483647)
p = torch.rand(3, generator=g)
p = p / p.sum()
p


torch.multinomial(p, num_samples=100, replacement=True, generator=g)


P = (N + 1).float()
P /= P.sum(axis=1, keepdim=True)


P.shape


P.sum(axis=1, keepdim=True).shape


# 27, 27
# 27, 1


P.sum(axis=1)


P[0].sum()


g = torch.Generator().manual_seed(2147483647)

for i in range(5):
    out = []
    ix = 0
    while True:
        p = P[ix]
        #p = N[ix].float()
        #p = p / p.sum()
        #p = torch.ones(33) / 33.0
        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
        out.append(itos[ix])
        if ix == 0:
            break
    print("".join(out))
    


# GOAL: maximize likelihood of the data w.r.t model parameters (statistical modeling)
# equivalent to maximizing the log likelihood (because log is monotonic)
# equivalent to minimizing the negative log likelihood
# equivalent to minimizing the average negative log likelihood 

# log(a*b*c) = log(a) + log(b) + log(c)


log_likelihood = 0.0
n = 0


for w in words:
#for w in ["mariq"]:
    chs = ["."] + list(w) + ["."]
    for ch1, ch2 in  zip(chs, chs[1:]):
        ix1 = stoi[ch1]
        ix2 = stoi[ch2]
        prob = P[ix1, ix2]
        logprob = torch.log(prob)
        log_likelihood += logprob
        n += 1 
        #print(f"{ch1}{ch2}: {prob:.4f} {logprob:.4f}")

print(f"{log_likelihood=}")
nll = -log_likelihood
print(f"{nll=}")
print(f"{nll / n}")





# Create a training set of bigrams (x,y)
xs, ys = [], []


for w in words[:1]:
    chs = ["."] + list(w) + ["."]
    for ch1, ch2 in  zip(chs, chs[1:]):
        ix1 = stoi[ch1]
        ix2 = stoi[ch2]
        print(ch1, ch2)
        xs.append(ix1)
        ys.append(ix2)

xs = torch.tensor(xs)
ys = torch.tensor(ys)


words[:1]


xs


ys


import torch.nn.functional as F
xenc = F.one_hot(xs, num_classes=33).float()
xenc


xenc.shape


plt.imshow(xenc);


xenc.dtype


W = torch.randn((33,33))
xenc @ W


(xenc @ W).shape


(xenc @ W)[3,13]


import numpy as np

np.dot(xenc[3],W[:, 13]) 


logits = xenc @ W #log-counts
counts = logits.exp() # Equivalent to the N matrix
prob = counts / counts.sum(1, keepdims=True)
prob


prob[0].sum()


def logit(p):
    return np.log(p / (1-p))

print(logit(0.001))
print(logit(0.5))
print(logit(0.999))


# SUMMARY ---------------------->>>>>


xs


ys


# Randomly Initialize 33 Neurons' Weights. Each neuron receives 33 inputs
g = torch.Generator().manual_seed(2147483647)
W = torch.randn((33,33), generator=g)


xenc = F.one_hot(xs, num_classes=33).float() # Input to the network: one-hot encoding
logits = xenc @ W # predict log-counts
counts = logits.exp() # Counts, Equivalent to N
probs = counts / counts.sum(1, keepdims=True) # Probavbilities for the next character
# btw: The last to lines is equivalent to doing the sofmax operation
torch.allclose(probs, F.softmax(logits, dim=1))


probs.shape


nlls = torch.zeros(5)
for i in range(5):
    # i-th bigram
    x = xs[i].item() # input character index
    y = ys[i].item() # label character index
    print("_________")
    print(f"Bigram example {i+1}: {itos[x]}{itos[y]} (indecies {x}, {y})")
    print("input to the neural net:", x)
    print("output probabilities from the neural net:", probs[i])
    print("label (actual next character):", y)
    p = probs[i, y]
    print("probability assigned by the net to the correct character:", p.item())
    logp = torch.log(p)
    print("log likelihod:", logp.item())
    nll = -logp
    print("negative log likelihood:", nll.item())
    nlls[i] = nll

print("==========")
print("average negative log likelihood, i.e. loss =", nlls.mean().item())


words[:1]


# ------- !!! OPTIMIZATION !!! -------


xs


ys


# Randomly Initialize 33 Neurons' Weights. Each neuron receives 33 inputs
g = torch.Generator().manual_seed(2147483647)
W = torch.randn((33,33), generator=g, requires_grad=True)


# Forward pass
xenc = F.one_hot(xs, num_classes=33).float() # Input to the network: one-hot encoding
logits = xenc @ W # predict log-counts
counts = logits.exp() # Counts, Equivalent to N
probs = counts / counts.sum(1, keepdims=True) # Probavbilities for the next character
loss = -probs[torch.arange(5), ys].log().mean() # negative mean log likelihood


# Backward pass (backpropagation)
W.grad = None # Set gradients to zero
loss.backward()


print(loss.item())


# Upgrade (Gradient Descent)
W.data += -0.1 * W.grad


probs.shape


probs [0,1], probs[1,2], probs[2,5], probs[3,12], probs[4,0]


probs[torch.arange(5), ys]


loss = -probs[torch.arange(5), ys].log().mean()
loss





xs, ys = [], []
for w in words:
    chs = ["."] + list(w) + ["."]
    for ch1, ch2 in  zip(chs, chs[1:]):
        ix1 = stoi[ch1]
        ix2 = stoi[ch2]
        xs.append(ix1)
        ys.append(ix2)
xs = torch.tensor(xs)
ys = torch.tensor(ys)
num = xs.nelement()
print("number of examples:", num)

#Initialize the network
g = torch.Generator().manual_seed(2147483647)
W = torch.randn((33,33), generator=g, requires_grad=True)


# Gradient Descent
for k in range(100):
    # Forward pass
    xenc = F.one_hot(xs, num_classes=33).float() # Input to the network: one-hot encoding
    logits = xenc @ W # predict log-counts
    counts = logits.exp() # Counts, Equivalent to N
    probs = counts / counts.sum(1, keepdims=True) # Probavbilities for the next character
    loss = -probs[torch.arange(num), ys].log().mean() # negative mean log likelihood
    print(loss.item())

        # Backward pass (backpropagation)
    W.grad = None # Set gradients to zero
    loss.backward()

    # Upgrade (Gradient Descent)
    W.data += -50 * W.grad
    



